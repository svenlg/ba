{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f881ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "#from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "#model_name = \"dbmdz/bert-base-german-cased\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cbea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PAD]  Padding token 512 tokens per seqences                          0\n",
    "# [UNK]  Used when a word is unknown to Bert                          100\n",
    "# [CLS]  Appears at the start of every sequence                       101\n",
    "# [SEP]  Indicates a seperator - between and end of sequences token   102\n",
    "# [MASK] Used when masking tokens, masked language modelling (MLM)    103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071404dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(ocn):\n",
    "    \n",
    "    input_ids = torch.from_numpy(np.load(ocn))\n",
    "    mask = torch.ones(input_ids.size())\n",
    "    \n",
    "    input_id_chunks = input_ids.split(510)\n",
    "    mask_chunks = mask.split(510)\n",
    "    \n",
    "    chunksize = 512\n",
    "    \n",
    "    input_id_chunks = list(input_id_chunks) \n",
    "    mask_chunks = list(mask_chunks) \n",
    "    \n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.Tensor([101]),input_id_chunks[i],torch.Tensor([102])\n",
    "        ])\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.Tensor([1]),mask_chunks[i],torch.Tensor([1])\n",
    "        ])\n",
    "        \n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        \n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            \n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            \n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attentions_mask = torch.stack(mask_chunks)\n",
    "    \n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long(),\n",
    "        'attention_mask': attentions_mask.int()\n",
    "    }\n",
    "    \n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ee5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_change_new(law):\n",
    "\n",
    "    law = str(law)\n",
    "    fname = '../Data_Laws/' + law + '/'\n",
    "    changes = np.loadtxt(fname + 'changes.txt', dtype=str, encoding='utf-8')\n",
    "    \n",
    "    ten_law = []\n",
    "    \n",
    "    if changes.shape == ():\n",
    "        change = str(changes)\n",
    "        old = get_tensors(fname + change + '/old.npy')\n",
    "        cha = get_tensors(fname + change + '/change.npy')\n",
    "        new = get_tensors(fname + change + '/new.npy')\n",
    "        ocn = (old,cha,new)\n",
    "        ten_law.append(ocn)\n",
    "        return ten_law\n",
    "    \n",
    "    for change in changes:\n",
    "        change = str(change)\n",
    "        \n",
    "        if law == 'KWG' and change == 'Nr7_2020-12-29':\n",
    "            continue\n",
    "            \n",
    "        old = get_tensors(fname + change + '/old.npy')\n",
    "        cha = get_tensors(fname + change + '/change.npy')\n",
    "        new = get_tensors(fname + change + '/new.npy')\n",
    "        ocn = (old,cha,new)\n",
    "        ten_law.append(ocn)\n",
    "        \n",
    "    return ten_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d63ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laws(split):\n",
    "    \n",
    "    assert 0 <= split <= 1\n",
    "\n",
    "    fname = '../Data_Laws/'\n",
    "    laws = np.loadtxt(fname + 'done_with.txt', dtype=str, encoding='utf-8')\n",
    "    ten = []\n",
    "    np.random.shuffle(laws)\n",
    "    num_data = int(split*len(laws))\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        print(laws[i])\n",
    "        ten.append(get_old_change_new(laws[i]))\n",
    "    \n",
    "    return ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223487d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG\n",
      "VwVfG\n",
      "VVG\n",
      "AsylG\n",
      "SGB_II\n",
      "BBG\n",
      "BRAO\n",
      "WpDVerOV\n",
      "UmwG\n",
      "GNotKG\n",
      "RVG\n",
      "HGB\n",
      "GWB\n",
      "KWKG_2020\n",
      "FinDAG\n",
      "KHEntgG\n",
      "JGG\n",
      "EGAktG\n",
      "BtMVV\n",
      "ArbZG\n",
      "SGB_IV\n",
      "EnWG\n",
      "BtMG\n",
      "EEWÃ¤rmeG\n",
      "ArbGG\n",
      "GKG\n"
     ]
    }
   ],
   "source": [
    "data = get_laws(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99862c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "26\n",
      "<class 'list'>\n",
      "4\n",
      "<class 'tuple'>\n",
      "3\n",
      "<class 'dict'>\n",
      "torch.Size([8, 512])\n",
      "tensor([[  101, 29977,  5223,  ...,   231, 10569,   102],\n",
      "        [  101,  7446,  9837,  ...,  9837, 22722,   102],\n",
      "        [  101, 26600,   222,  ...,   211,  7644,   102],\n",
      "        ...,\n",
      "        [  101,   197,  9058,  ...,   566,   493,   102],\n",
      "        [  101,   566,  1073,  ...,   216,   128,   102],\n",
      "        [  101,  3358,   222,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#print(type(data)) # list: Gesetzten die genutzt werden\n",
    "#print(len(data))  # int(split*len(laws))\n",
    "\n",
    "#print(type(data[0])) # list: Changes die es gab pro Gesetz\n",
    "#print(len(data[0]))  # Num an Changes\n",
    "      \n",
    "#print(type(data[0][0])) # tuple: old, change, new\n",
    "#print(len(data[0][0]))  # 3\n",
    "\n",
    "#print(type(data[0][0][0])) # dict: key: ('input_ids', 'attention_mask') values: there pt_tensor representation\n",
    "\n",
    "#print(data[0][0][0]['input_ids'].shape) #shape = (__,512)\n",
    "#print(data[0][0][0]['input_ids']) #pt_tensor long: attual data\n",
    "#print(data[0][0][0]['attention_mask']) #pt_tensor int: only 1 (attention) or 0 (no attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(**input_dict)\n",
    "#\n",
    "#outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcaea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old, change, new = get_data(0.02)\n",
    "\n",
    "#train_old, val_old, train_change, val_change, train_new, val_new = train_test_split(old, change, new, test_size=.5)\n",
    "\n",
    "\n",
    "# tr_enc_change = tokenizer(train_change[0], truncation=True, padding=True)\n",
    "# print(type(tr_enc_change))\n",
    "# tr_enc_new = tokenizer(train_new[0], truncation=True, padding=True)\n",
    "# print(tr_enc_change.keys())\n",
    "# val_enc_old = tokenizer(val_old, truncation=True, padding=True)\n",
    "# val_enc_change = tokenizer(val_change, truncation=True, padding=True)\n",
    "# val_enc_new = tokenizer(val_new, truncation=True, padding=True)\n",
    "# test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# train_dataset = LawDataset(tr_enc_old, tr_enc_change, tr_enc_new)\n",
    "# val_dataset = LawDataset(val_enc_old, val_enc_change, val_enc_new)\n",
    "# test_dataset = LawDataset(test_encodings, test_labels)\n",
    "\n",
    "# print('So weit geht es!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
